{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters Graph version\n",
    "\n",
    "### Sources\n",
    "- [Philipp tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html)\n",
    "- [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "- [nltk docs](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/floor_vl/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
=======
      "[nltk_data] Downloading package reuters to /home/matyi/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
>>>>>>> main
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from data_prep.reuters_graph import R8Graph, R52Graph\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/floor_vl/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from datasets.graph_utils import PMI, tf_idf_mtx\n",
    "\n",
    "def prepare_reuters(r8=False):\n",
    "    \"\"\"Filters out all documents which have more or less than 1 class. Then filters out all classes which have no remaining documents.\n",
    "\n",
    "    Args:\n",
    "        r8 (bool, optional): R8 is constructed by taking only the top 10 (original) classes. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        train_docs (list): List of training documents.\n",
    "        test_docs (list): List of test documents.\n",
    "    \"\"\"    \n",
    "    # Filter out docs which don't have exactly 1 class\n",
    "    data = defaultdict(lambda: {'train': [], 'test': []})\n",
    "    for doc in reuters.fileids():\n",
    "        # print(\"reuter field=\", doc)\n",
    "        if len(reuters.categories(doc)) == 1:\n",
    "            if doc.startswith('training'):\n",
    "                data[reuters.categories(doc)[0]]['train'].append(doc)\n",
    "            elif doc.startswith('test'):\n",
    "                data[reuters.categories(doc)[0]]['test'].append(doc)\n",
    "            else:\n",
    "                print(doc)\n",
    "\n",
    "    # Filter out classes which have no remaining docs\n",
    "    for cls in reuters.categories():\n",
    "        if len(data[cls]['train']) < 1 or len(data[cls]['test']) < 1:\n",
    "            data.pop(cls, None)\n",
    "\n",
    "    if r8:\n",
    "        # Choose top 10 classes and then select the ones which still remain after filtering\n",
    "        popular = sorted(reuters.categories(), key=lambda cls: len(reuters.fileids(cls)), reverse=True)[:10]\n",
    "        data = dict([(cls, splits) for (cls, splits) in data.items() if cls in popular])\n",
    "\n",
    "    # Create splits\n",
    "    train_docs = [doc for cls, splits in data.items() for doc in splits['train']]\n",
    "    test_docs = [doc for cls, splits in data.items() for doc in splits['test']]\n",
    "\n",
    "    return train_docs, test_docs, list(data.keys())\n",
    "\n",
    "class Reuters:\n",
    "    def __init__(self, r8, device, val_size=0.1):\n",
    "        self.device = device\n",
    "        print('Prepare Reuters dataset')\n",
    "        train_docs, test_docs, classes = prepare_reuters(r8)\n",
    "        corpus = [[word.lower() for word in reuters.words(doc)] for doc in train_docs + test_docs]\n",
    "        \n",
    "        print('Compute tf.idf')\n",
    "        tf_idf, words = tf_idf_mtx(corpus)\n",
    "        \n",
    "        print('Compute PMI scores')\n",
    "        pmi = PMI(corpus)\n",
    "        \n",
    "        # Index to node name mapping\n",
    "        self.iton = list(train_docs + test_docs + words)\n",
    "        # Node name to index mapping\n",
    "        self.ntoi = {self.iton[i]: i for i in range(len(self.iton))}\n",
    "        \n",
    "        # Edge index and values for dataset\n",
    "        print('Generate edges')\n",
    "        edge_index, edge_attr = self.generate_edges(len(train_docs + test_docs), tf_idf, pmi)\n",
    "        \n",
    "        # Index to label mapping\n",
    "        self.itol = classes\n",
    "        # Label in index mapping\n",
    "        self.loti = {self.itol[i]: i for i in range(len(self.itol))}\n",
    "        # Labels to node mapping, where word nodes get the label of -1\n",
    "        ntol = [self.loti[reuters.categories(node)[0]] if reuters.categories(node) else -1 for node in self.iton]\n",
    "        ntol = torch.tensor(ntol, device=device)\n",
    "        \n",
    "        # Generate masks/splits\n",
    "        print('Generate masks')\n",
    "        train_mask, val_mask, test_mask = self.generate_masks(len(train_docs), len(test_docs), val_size)\n",
    "        \n",
    "        # Feature matrix is Identity (according to TextGCN)\n",
    "        print('Generate feature matrix')\n",
    "        node_feats = torch.eye(len(self.iton), device=self.device)\n",
    "        print('Features mtx is {} GBs in size'.format(node_feats.nelement() * node_feats.element_size() * 1e-9))\n",
    "        \n",
    "        # Create pytorch geometric format data\n",
    "        self.data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_attr, y=ntol)\n",
    "        self.data.train_mask = train_mask\n",
    "        self.data.val_mask = val_mask\n",
    "        self.data.test_mask = test_mask\n",
    "        \n",
    "    def generate_edges(self, num_docs, tf_idf, pmi):\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        # Document-word edges\n",
    "        for d_ind, doc in enumerate(tf_idf):\n",
    "            word_inds = doc.indices\n",
    "            for w_ind in word_inds:\n",
    "                edge_index.append([d_ind, num_docs + w_ind])\n",
    "                edge_index.append([num_docs + w_ind, d_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "        \n",
    "        # Word-word edges\n",
    "        for (word_i, word_j), score in pmi.items():\n",
    "            w_i_ind = self.ntoi[word_i]\n",
    "            w_j_ind = self.ntoi[word_j]\n",
    "            edge_index.append([w_i_ind, w_j_ind])\n",
    "            edge_index.append([w_j_ind, w_i_ind])\n",
    "            edge_attr.append(score)\n",
    "            edge_attr.append(score)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr)\n",
    "        return edge_index, edge_attr\n",
    "    \n",
    "    def generate_masks(self, train_num, test_num, val_size):\n",
    "        # Mask all non-training docs\n",
    "        train_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        train_mask[:train_num] = 1\n",
    "        \n",
    "        # Randomly select val docs from train-docs and mask accordingly\n",
    "        val_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        val_mask_inds = torch.randperm(train_num)[:int(val_size * train_num)]\n",
    "        val_mask[val_mask_inds] = 1\n",
    "        train_mask[val_mask_inds] = 0\n",
    "        \n",
    "        # Mask all non-test docs\n",
    "        test_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        test_mask[train_num:test_num] = 1\n",
    "        test_mask = train_mask.bool()\n",
    "\n",
    "        return train_mask.bool(), val_mask.bool(), test_mask.bool()\n",
    "        \n",
    "        \n",
    "class R52(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=False, device=device, val_size=val_size)\n",
    "        \n",
    "        \n",
    "class R8(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=True, device=device, val_size=val_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
   "id": "c7a5e435",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare Reuters dataset\n",
      "Compute tf.idf\n",
      "Compute PMI scores\n",
      "Generate edges\n",
      "Generate masks\n",
      "Generate feature matrix\n",
      "Features mtx is 0.584769124 GBs in size\n"
     ]
    }
   ],
   "source": [
    "r8 = R8Graph(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[4547910], edge_index=[2, 4547910], test_mask=[32966], train_mask=[32966], val_mask=[32966], x=[32966, 32966], y=[32966])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r8.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r8.data.val_mask * r8.data.train_mask * r8.data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.utils.is_undirected(r8.data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
=======
   "execution_count": 3,
   "id": "bc94fbc5",
   "metadata": {},
   "outputs": [
    {
>>>>>>> main
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "(array([0, 1, 2, 4, 5, 7]), array([35,  5, 49,  2,  4,  5]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(r8.data.y[r8.data.train_mask].cpu(), return_counts=True))\n",
    "print(np.unique(r8.data.y[r8.data.val_mask].cpu(), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
=======
   "execution_count": 4,
   "id": "b67071f0",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(1000, device='cuda:0')\n",
      "tensor(100, device='cuda:0')\n",
      "tensor(100, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sum(r8.data.val_mask * r8.data.train_mask * r8.data.test_mask))\n",
    "print(sum(r8.data.train_mask))\n",
    "print(sum(r8.data.val_mask))\n",
    "print(sum(r8.data.test_mask))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 5,
   "id": "512c809e",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphConv, GATConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GraphConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(len(r8.iton), 200)\n",
    "        self.conv2 = GCNConv(200, 8)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "def eval(model, data, mask):\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    #print(pred[mask])\n",
    "    #print(data.y[mask])\n",
    "    correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "    acc = correct / mask.sum()\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 6,
   "id": "4726a0ab",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[1209458], edge_index=[2, 1209458], test_mask=[12091], train_mask=[12091], val_mask=[12091], x=[12091, 12091], y=[12091])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 6,
>>>>>>> main
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)#, weight_decay=5e-4)\n",
    "r8.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, data, data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
=======
   "execution_count": 7,
   "id": "307e3254",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daafe1d3219845d190348b2d0cab08fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0793890953063965\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([522, 126,  35,  60,   9,   3, 220,  25]))\n",
      "Accuracy: 0.4900\n",
      "Loss: 1.600253701210022\n",
      "(array([2]), array([1000]))\n",
      "Accuracy: 0.4900\n",
      "Loss: 1.4529918432235718\n",
      "(array([2]), array([1000]))\n",
      "Accuracy: 0.4900\n",
      "Loss: 1.38367760181427\n",
      "(array([2]), array([1000]))\n",
      "Accuracy: 0.5200\n",
      "Loss: 1.2150911092758179\n",
      "(array([0, 2]), array([ 34, 966]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 1.09518563747406\n",
      "(array([0, 2]), array([467, 533]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.9767016172409058\n",
      "(array([0, 2]), array([473, 527]))\n",
      "Accuracy: 0.8300\n",
      "Loss: 0.8734109401702881\n",
      "(array([0, 2]), array([443, 557]))\n",
      "Accuracy: 0.8100\n",
      "Loss: 0.7946096062660217\n",
      "(array([0, 2]), array([454, 546]))\n",
      "Accuracy: 0.8300\n",
      "Loss: 0.7308882474899292\n",
      "(array([0, 2, 7]), array([469, 530,   1]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.6623002290725708\n",
      "(array([0, 2, 7]), array([428, 522,  50]))\n",
      "Accuracy: 0.8500\n",
      "Loss: 0.5993506908416748\n",
      "(array([0, 2, 7]), array([290, 536, 174]))\n",
      "Accuracy: 0.8600\n",
      "Loss: 0.5443897843360901\n",
      "(array([0, 2, 7]), array([271, 512, 217]))\n",
      "Accuracy: 0.8400\n",
      "Loss: 0.5081155300140381\n",
      "(array([0, 2, 7]), array([262, 500, 238]))\n",
      "Accuracy: 0.8400\n",
      "Loss: 0.4565276503562927\n",
      "(array([0, 1, 2, 7]), array([266,   4, 489, 241]))\n",
      "Accuracy: 0.8500\n",
      "Loss: 0.4083865284919739\n",
      "(array([0, 1, 2, 4, 5, 7]), array([262,   8, 502,   1,   4, 223]))\n",
      "Accuracy: 0.8700\n",
      "Loss: 0.3614181578159332\n",
      "(array([0, 1, 2, 4, 5, 7]), array([266,  26, 511,   6,   6, 185]))\n",
      "Accuracy: 0.8900\n",
      "Loss: 0.3147873878479004\n",
      "(array([0, 1, 2, 4, 5, 6, 7]), array([266,  37, 493,  17,  16,   8, 163]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.27088817954063416\n",
      "(array([0, 1, 2, 4, 5, 6, 7]), array([264,  39, 471,  35,  24,  22, 145]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.22516481578350067\n",
      "(array([0, 1, 2, 4, 5, 6, 7]), array([263,  41, 465,  45,  27,  28, 131]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.19333335757255554\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  38, 465,   2,  48,  28,  35, 122]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.1469232589006424\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  40, 464,   7,  43,  30,  33, 121]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.1152929738163948\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  40, 462,  12,  38,  33,  35, 118]))\n",
      "Accuracy: 0.9300\n",
      "Loss: 0.09889023005962372\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  40, 462,  13,  37,  37,  36, 113]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.07974187284708023\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  41, 462,  13,  35,  41,  36, 110]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.05808749794960022\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  41, 462,  14,  37,  39,  36, 109]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.042655523866415024\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  36,  40,  36, 108]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.033258408308029175\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9100\n",
      "Loss: 0.02585248276591301\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 462,  13,  39,  39,  36, 107]))\n",
      "Accuracy: 0.9300\n",
      "Loss: 0.021373216062784195\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  38,  38,  36, 108]))\n",
      "Accuracy: 0.8900\n",
      "Loss: 0.01984309032559395\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  40,  36,  36, 108]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.011970704421401024\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9100\n",
      "Loss: 0.008332813158631325\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9100\n",
      "Loss: 0.006258639972656965\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.005049710627645254\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.8900\n",
      "Loss: 0.00466955779120326\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.003086480777710676\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9400\n",
      "Loss: 0.002484685741364956\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.0020905639976263046\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.002331819850951433\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([262,  42, 461,  15,  39,  38,  36, 107]))\n",
      "Accuracy: 0.9100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(40)):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(r8.data)\n",
    "    # We might want to use the \"weight\" parameter for the loss with unbalanced dataset\n",
    "    # since with a low learning rate the model just assigns every doc to class \"earn\"\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    loss = F.cross_entropy(out[r8.data.train_mask], r8.data.y[r8.data.train_mask])\n",
    "    print('Loss:', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(np.unique(out[r8.data.train_mask].max(dim=1)[1].detach().cpu().numpy(), return_counts=True))\n",
    "    #print(np.unique(r8.data.y[r8.data.train_mask].detach().cpu().numpy(), return_counts=True))\n",
    "    eval(model, r8.data, r8.data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 8,
   "id": "bf770635",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy: 0.2894\n"
     ]
    }
   ],
   "source": [
    "eval(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
=======
      "Accuracy: 0.9000\n"
>>>>>>> main
=======
      "Accuracy: 0.9200\n"
>>>>>>> main
     ]
    }
   ],
   "source": [
    "eval(model, r8.data, r8.data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "71d5323e",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.3"
=======
   "version": "3.8.8"
>>>>>>> main
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
