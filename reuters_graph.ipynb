{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dcea768",
   "metadata": {},
   "source": [
    "# Reuters Graph version\n",
    "\n",
    "### Sources\n",
    "- [Philipp tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html)\n",
    "- [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "- [nltk docs](https://www.nltk.org/book/ch02.html)\n",
    "\n",
    "### TODO\n",
    "- make edge_list a set and then a list again\n",
    "- edges: tf.idf and word-word PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5373b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.cuda(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6d3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from datasets.graph_utils import PMI, tf_idf_mtx\n",
    "\n",
    "def prepare_reuters(r8=False):\n",
    "    \"\"\"Filters out all documents which have more or less than 1 class. Then filters out all classes which have no remaining documents.\n",
    "\n",
    "    Args:\n",
    "        r8 (bool, optional): R8 is constructed by taking only the top 10 (original) classes. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        train_docs (list): List of training documents.\n",
    "        test_docs (list): List of test documents.\n",
    "    \"\"\"    \n",
    "    # Filter out docs which don't have exactly 1 class\n",
    "    data = defaultdict(lambda: {'train': [], 'test': []})\n",
    "    for doc in reuters.fileids():\n",
    "        # print(\"reuter field=\", doc)\n",
    "        if len(reuters.categories(doc)) == 1:\n",
    "            if doc.startswith('training'):\n",
    "                data[reuters.categories(doc)[0]]['train'].append(doc)\n",
    "            elif doc.startswith('test'):\n",
    "                data[reuters.categories(doc)[0]]['test'].append(doc)\n",
    "            else:\n",
    "                print(doc)\n",
    "\n",
    "    # Filter out classes which have no remaining docs\n",
    "    for cls in reuters.categories():\n",
    "        if len(data[cls]['train']) < 1 or len(data[cls]['test']) < 1:\n",
    "            data.pop(cls, None)\n",
    "\n",
    "    if r8:\n",
    "        # Choose top 10 classes and then select the ones which still remain after filtering\n",
    "        popular = sorted(reuters.categories(), key=lambda cls: len(reuters.fileids(cls)), reverse=True)[:10]\n",
    "        data = dict([(cls, splits) for (cls, splits) in data.items() if cls in popular])\n",
    "\n",
    "    # Create splits\n",
    "    train_docs = [doc for cls, splits in data.items() for doc in splits['train']]\n",
    "    test_docs = [doc for cls, splits in data.items() for doc in splits['test']]\n",
    "\n",
    "    return train_docs, test_docs, list(data.keys())\n",
    "\n",
    "class Reuters:\n",
    "    def __init__(self, r8, device, val_size=0.1):\n",
    "        self.device = device\n",
    "        print('Prepare Reuters dataset')\n",
    "        train_docs, test_docs, classes = prepare_reuters(r8)\n",
    "        corpus = [[word.lower() for word in reuters.words(doc)] for doc in train_docs + test_docs]\n",
    "        \n",
    "        print('Compute tf.idf')\n",
    "        tf_idf, words = tf_idf_mtx(corpus)\n",
    "        \n",
    "        print('Compute PMI scores')\n",
    "        pmi = PMI(corpus)\n",
    "        \n",
    "        # Index to node name mapping\n",
    "        self.iton = list(train_docs + test_docs + words)\n",
    "        # Node name to index mapping\n",
    "        self.ntoi = {self.iton[i]: i for i in range(len(self.iton))}\n",
    "        \n",
    "        # Edge index and values for dataset\n",
    "        print('Generate edges')\n",
    "        edge_index, edge_attr = self.generate_edges(len(train_docs + test_docs), tf_idf, pmi)\n",
    "        \n",
    "        # Index to label mapping\n",
    "        self.itol = classes\n",
    "        # Label in index mapping\n",
    "        self.loti = {self.itol[i]: i for i in range(len(self.itol))}\n",
    "        # Labels to node mapping, where word nodes get the label of -1\n",
    "        ntol = [self.loti[reuters.categories(node)[0]] if reuters.categories(node) else -1 for node in self.iton]\n",
    "        ntol = torch.tensor(ntol, device=device)\n",
    "        \n",
    "        # Generate masks/splits\n",
    "        print('Generate masks')\n",
    "        train_mask, val_mask, test_mask = self.generate_masks(len(train_docs), len(test_docs), val_size)\n",
    "        \n",
    "        # Feature matrix is Identity (according to TextGCN)\n",
    "        print('Generate feature matrix')\n",
    "        node_feats = torch.eye(len(self.iton), device=self.device)\n",
    "        print('Features mtx is {} GBs in size'.format(node_feats.nelement() * node_feats.element_size() * 1e-9))\n",
    "        \n",
    "        # Create pytorch geometric format data\n",
    "        self.data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_attr, y=ntol)\n",
    "        self.data.train_mask = train_mask\n",
    "        self.data.val_mask = val_mask\n",
    "        self.data.test_mask = test_mask\n",
    "        \n",
    "    def generate_edges(self, num_docs, tf_idf, pmi):\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        # Document-word edges\n",
    "        for d_ind, doc in enumerate(tf_idf):\n",
    "            word_inds = doc.indices\n",
    "            for w_ind in word_inds:\n",
    "                edge_index.append([d_ind, num_docs + w_ind])\n",
    "                edge_index.append([num_docs + w_ind, d_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "        \n",
    "        # Word-word edges\n",
    "        for (word_i, word_j), score in pmi.items():\n",
    "            w_i_ind = self.ntoi[word_i]\n",
    "            w_j_ind = self.ntoi[word_j]\n",
    "            edge_index.append([w_i_ind, w_j_ind])\n",
    "            edge_index.append([w_j_ind, w_i_ind])\n",
    "            edge_attr.append(score)\n",
    "            edge_attr.append(score)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr)\n",
    "        return edge_index, edge_attr\n",
    "    \n",
    "    def generate_masks(self, train_num, test_num, val_size):\n",
    "        # Mask all non-training docs\n",
    "        train_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        train_mask[:train_num] = 1\n",
    "        \n",
    "        # Randomly select val docs from train-docs and mask accordingly\n",
    "        val_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        val_mask_inds = torch.randperm(train_num)[:int(val_size * train_num)]\n",
    "        val_mask[val_mask_inds] = 1\n",
    "        train_mask[val_mask_inds] = 0\n",
    "        \n",
    "        # Mask all non-test docs\n",
    "        test_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        test_mask[train_num:test_num] = 1\n",
    "        test_mask = train_mask.bool()\n",
    "\n",
    "        return train_mask.bool(), val_mask.bool(), test_mask.bool()\n",
    "        \n",
    "        \n",
    "class R52(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=False, device=device, val_size=val_size)\n",
    "        \n",
    "        \n",
    "class R8(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=True, device=device, val_size=val_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a5e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare Reuters dataset\n",
      "Compute tf.idf\n",
      "Compute PMI scores\n",
      "Generate edges\n",
      "Generate masks\n",
      "Generate feature matrix\n",
      "Features mtx is 4.347028624 GBs in size\n"
     ]
    }
   ],
   "source": [
    "r8 = R8(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc94fbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[4547910], edge_index=[2, 4547910], test_mask=[32966], train_mask=[32966], val_mask=[32966], x=[32966, 32966], y=[32966])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r8.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67071f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r8.data.val_mask * r8.data.train_mask * r8.data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aab622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca96a8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.utils.is_undirected(r8.data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22836344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cora_dataset = torch_geometric.datasets.Planetoid(root='/tmp/cora', name=\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7330e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efbb9e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512c809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(len(r8.iton), 8)\n",
    "        self.conv2 = GCNConv(8, 8)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x.double(), data.edge_index, data.edge_attr\n",
    "        print(x.type())\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        print(x.type())\n",
    "        x = F.relu(x)\n",
    "        print(x.type())\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        print(x.type())\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        print(x.type())\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def eval(model, data, mask):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = int(pred[mask].eq(data.y[mask]).sum().item())\n",
    "    acc = correct / int(mask.sum())\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4726a0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[4547910], edge_index=[2, 4547910], test_mask=[32966], train_mask=[32966], val_mask=[32966], x=[32966, 32966], y=[32966])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "data = r8.data\n",
    "\n",
    "r8.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f96a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 8.10 GiB (GPU 0; 5.93 GiB total capacity; 4.15 GiB already allocated; 751.81 MiB free; 4.17 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a994c88cd016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b4cbf406a818>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, data, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/university/ATCS_group1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b4cbf406a818>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 8.10 GiB (GPU 0; 5.93 GiB total capacity; 4.15 GiB already allocated; 751.81 MiB free; 4.17 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "eval(model, data, data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "307e3254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d351e6c963b4dcdaa36ad526198c416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    eval(model, data, data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf770635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2894\n"
     ]
    }
   ],
   "source": [
    "eval(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20775fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d07a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
