{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters Graph version\n",
    "\n",
    "### Sources\n",
    "- [Philipp tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html)\n",
    "- [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "- [nltk docs](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/floor_vl/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
=======
      "[nltk_data] Downloading package reuters to /home/matyi/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
>>>>>>> main
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets.reuters_graph import R8, R52\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/floor_vl/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from datasets.graph_utils import PMI, tf_idf_mtx\n",
    "\n",
    "def prepare_reuters(r8=False):\n",
    "    \"\"\"Filters out all documents which have more or less than 1 class. Then filters out all classes which have no remaining documents.\n",
    "\n",
    "    Args:\n",
    "        r8 (bool, optional): R8 is constructed by taking only the top 10 (original) classes. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        train_docs (list): List of training documents.\n",
    "        test_docs (list): List of test documents.\n",
    "    \"\"\"    \n",
    "    # Filter out docs which don't have exactly 1 class\n",
    "    data = defaultdict(lambda: {'train': [], 'test': []})\n",
    "    for doc in reuters.fileids():\n",
    "        # print(\"reuter field=\", doc)\n",
    "        if len(reuters.categories(doc)) == 1:\n",
    "            if doc.startswith('training'):\n",
    "                data[reuters.categories(doc)[0]]['train'].append(doc)\n",
    "            elif doc.startswith('test'):\n",
    "                data[reuters.categories(doc)[0]]['test'].append(doc)\n",
    "            else:\n",
    "                print(doc)\n",
    "\n",
    "    # Filter out classes which have no remaining docs\n",
    "    for cls in reuters.categories():\n",
    "        if len(data[cls]['train']) < 1 or len(data[cls]['test']) < 1:\n",
    "            data.pop(cls, None)\n",
    "\n",
    "    if r8:\n",
    "        # Choose top 10 classes and then select the ones which still remain after filtering\n",
    "        popular = sorted(reuters.categories(), key=lambda cls: len(reuters.fileids(cls)), reverse=True)[:10]\n",
    "        data = dict([(cls, splits) for (cls, splits) in data.items() if cls in popular])\n",
    "\n",
    "    # Create splits\n",
    "    train_docs = [doc for cls, splits in data.items() for doc in splits['train']]\n",
    "    test_docs = [doc for cls, splits in data.items() for doc in splits['test']]\n",
    "\n",
    "    return train_docs, test_docs, list(data.keys())\n",
    "\n",
    "class Reuters:\n",
    "    def __init__(self, r8, device, val_size=0.1):\n",
    "        self.device = device\n",
    "        print('Prepare Reuters dataset')\n",
    "        train_docs, test_docs, classes = prepare_reuters(r8)\n",
    "        corpus = [[word.lower() for word in reuters.words(doc)] for doc in train_docs + test_docs]\n",
    "        \n",
    "        print('Compute tf.idf')\n",
    "        tf_idf, words = tf_idf_mtx(corpus)\n",
    "        \n",
    "        print('Compute PMI scores')\n",
    "        pmi = PMI(corpus)\n",
    "        \n",
    "        # Index to node name mapping\n",
    "        self.iton = list(train_docs + test_docs + words)\n",
    "        # Node name to index mapping\n",
    "        self.ntoi = {self.iton[i]: i for i in range(len(self.iton))}\n",
    "        \n",
    "        # Edge index and values for dataset\n",
    "        print('Generate edges')\n",
    "        edge_index, edge_attr = self.generate_edges(len(train_docs + test_docs), tf_idf, pmi)\n",
    "        \n",
    "        # Index to label mapping\n",
    "        self.itol = classes\n",
    "        # Label in index mapping\n",
    "        self.loti = {self.itol[i]: i for i in range(len(self.itol))}\n",
    "        # Labels to node mapping, where word nodes get the label of -1\n",
    "        ntol = [self.loti[reuters.categories(node)[0]] if reuters.categories(node) else -1 for node in self.iton]\n",
    "        ntol = torch.tensor(ntol, device=device)\n",
    "        \n",
    "        # Generate masks/splits\n",
    "        print('Generate masks')\n",
    "        train_mask, val_mask, test_mask = self.generate_masks(len(train_docs), len(test_docs), val_size)\n",
    "        \n",
    "        # Feature matrix is Identity (according to TextGCN)\n",
    "        print('Generate feature matrix')\n",
    "        node_feats = torch.eye(len(self.iton), device=self.device)\n",
    "        print('Features mtx is {} GBs in size'.format(node_feats.nelement() * node_feats.element_size() * 1e-9))\n",
    "        \n",
    "        # Create pytorch geometric format data\n",
    "        self.data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_attr, y=ntol)\n",
    "        self.data.train_mask = train_mask\n",
    "        self.data.val_mask = val_mask\n",
    "        self.data.test_mask = test_mask\n",
    "        \n",
    "    def generate_edges(self, num_docs, tf_idf, pmi):\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        # Document-word edges\n",
    "        for d_ind, doc in enumerate(tf_idf):\n",
    "            word_inds = doc.indices\n",
    "            for w_ind in word_inds:\n",
    "                edge_index.append([d_ind, num_docs + w_ind])\n",
    "                edge_index.append([num_docs + w_ind, d_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "        \n",
    "        # Word-word edges\n",
    "        for (word_i, word_j), score in pmi.items():\n",
    "            w_i_ind = self.ntoi[word_i]\n",
    "            w_j_ind = self.ntoi[word_j]\n",
    "            edge_index.append([w_i_ind, w_j_ind])\n",
    "            edge_index.append([w_j_ind, w_i_ind])\n",
    "            edge_attr.append(score)\n",
    "            edge_attr.append(score)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr)\n",
    "        return edge_index, edge_attr\n",
    "    \n",
    "    def generate_masks(self, train_num, test_num, val_size):\n",
    "        # Mask all non-training docs\n",
    "        train_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        train_mask[:train_num] = 1\n",
    "        \n",
    "        # Randomly select val docs from train-docs and mask accordingly\n",
    "        val_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        val_mask_inds = torch.randperm(train_num)[:int(val_size * train_num)]\n",
    "        val_mask[val_mask_inds] = 1\n",
    "        train_mask[val_mask_inds] = 0\n",
    "        \n",
    "        # Mask all non-test docs\n",
    "        test_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        test_mask[train_num:test_num] = 1\n",
    "        test_mask = train_mask.bool()\n",
    "\n",
    "        return train_mask.bool(), val_mask.bool(), test_mask.bool()\n",
    "        \n",
    "        \n",
    "class R52(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=False, device=device, val_size=val_size)\n",
    "        \n",
    "        \n",
    "class R8(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=True, device=device, val_size=val_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
   "id": "c7a5e435",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare Reuters dataset\n",
      "Compute tf.idf\n",
      "Compute PMI scores\n",
      "Generate edges\n",
      "Generate masks\n",
      "Generate feature matrix\n",
      "Features mtx is 0.580713604 GBs in size\n"
     ]
    }
   ],
   "source": [
    "r8 = R8(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[4547910], edge_index=[2, 4547910], test_mask=[32966], train_mask=[32966], val_mask=[32966], x=[32966, 32966], y=[32966])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r8.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r8.data.val_mask * r8.data.train_mask * r8.data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.utils.is_undirected(r8.data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
=======
   "execution_count": 3,
   "id": "bc94fbc5",
   "metadata": {},
   "outputs": [
    {
>>>>>>> main
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "(array([0, 1, 2, 4, 5, 6, 7]), array([31,  3, 56,  1,  4,  3,  2]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(r8.data.y[r8.data.train_mask].cpu(), return_counts=True))\n",
    "print(np.unique(r8.data.y[r8.data.val_mask].cpu(), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
=======
   "execution_count": 4,
   "id": "b67071f0",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(1000, device='cuda:0')\n",
      "tensor(100, device='cuda:0')\n",
      "tensor(100, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sum(r8.data.val_mask * r8.data.train_mask * r8.data.test_mask))\n",
    "print(sum(r8.data.train_mask))\n",
    "print(sum(r8.data.val_mask))\n",
    "print(sum(r8.data.test_mask))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 5,
   "id": "512c809e",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphConv, GATConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GraphConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(len(r8.iton), 200)\n",
    "        self.conv2 = GCNConv(200, 8)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "def eval(model, data, mask):\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    #print(pred[mask])\n",
    "    #print(data.y[mask])\n",
    "    correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "    acc = correct / mask.sum()\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 6,
   "id": "4726a0ab",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[1208572], edge_index=[2, 1208572], test_mask=[12049], train_mask=[12049], val_mask=[12049], x=[12049, 12049], y=[12049])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 6,
>>>>>>> main
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)#, weight_decay=5e-4)\n",
    "r8.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, data, data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
=======
   "execution_count": 7,
   "id": "307e3254",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c034ec5141849af9abc4f2391aaae13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.079289436340332\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([ 56,  10, 285, 430, 161,  12,  19,  27]))\n",
      "Accuracy: 0.5600\n",
      "Loss: 1.577468752861023\n",
      "(array([2]), array([1000]))\n",
      "Accuracy: 0.5600\n",
      "Loss: 1.3939603567123413\n",
      "(array([2]), array([1000]))\n",
      "Accuracy: 0.5600\n",
      "Loss: 1.3254753351211548\n",
      "(array([2]), array([1000]))\n",
      "Accuracy: 0.6400\n",
      "Loss: 1.1599255800247192\n",
      "(array([0, 2]), array([194, 806]))\n",
      "Accuracy: 0.8000\n",
      "Loss: 1.0375745296478271\n",
      "(array([0, 2]), array([481, 519]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.9047172665596008\n",
      "(array([0, 2, 7]), array([444, 555,   1]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.8084975481033325\n",
      "(array([0, 2, 7]), array([417, 577,   6]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.7237847447395325\n",
      "(array([0, 2, 7]), array([410, 563,  27]))\n",
      "Accuracy: 0.8400\n",
      "Loss: 0.6571189165115356\n",
      "(array([0, 2, 7]), array([404, 533,  63]))\n",
      "Accuracy: 0.8600\n",
      "Loss: 0.5918500423431396\n",
      "(array([0, 2, 7]), array([333, 515, 152]))\n",
      "Accuracy: 0.8400\n",
      "Loss: 0.5329527854919434\n",
      "(array([0, 2, 7]), array([281, 498, 221]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.4884302616119385\n",
      "(array([0, 2, 7]), array([270, 497, 233]))\n",
      "Accuracy: 0.8200\n",
      "Loss: 0.4464576542377472\n",
      "(array([0, 2, 7]), array([264, 502, 234]))\n",
      "Accuracy: 0.8300\n",
      "Loss: 0.3980768322944641\n",
      "(array([0, 2, 5, 7]), array([265, 512,   1, 222]))\n",
      "Accuracy: 0.8300\n",
      "Loss: 0.35413694381713867\n",
      "(array([0, 1, 2, 5, 7]), array([266,  12, 509,   4, 209]))\n",
      "Accuracy: 0.8400\n",
      "Loss: 0.3101828694343567\n",
      "(array([0, 1, 2, 3, 5, 7]), array([264,  28, 505,   1,  11, 191]))\n",
      "Accuracy: 0.8700\n",
      "Loss: 0.27376043796539307\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  39, 492,   1,   7,  19,  14, 163]))\n",
      "Accuracy: 0.8900\n",
      "Loss: 0.23571081459522247\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([267,  39, 482,   2,  18,  22,  16, 154]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.19182945787906647\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([267,  39, 470,   9,  18,  28,  23, 146]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.15976466238498688\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  39, 472,   9,  20,  26,  23, 146]))\n",
      "Accuracy: 0.8800\n",
      "Loss: 0.12499886751174927\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  39, 470,  11,  24,  29,  27, 135]))\n",
      "Accuracy: 0.8800\n",
      "Loss: 0.1064835786819458\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([266,  40, 471,  15,  20,  34,  29, 125]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.07896656543016434\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 471,  17,  22,  33,  29, 122]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.061553895473480225\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 471,  15,  22,  36,  29, 121]))\n",
      "Accuracy: 0.9100\n",
      "Loss: 0.048036783933639526\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 472,  14,  24,  34,  29, 121]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.03845791146159172\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 472,  15,  23,  34,  29, 121]))\n",
      "Accuracy: 0.8800\n",
      "Loss: 0.029930079355835915\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 471,  15,  24,  34,  29, 121]))\n",
      "Accuracy: 0.8700\n",
      "Loss: 0.021736595779657364\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 471,  15,  24,  34,  29, 121]))\n",
      "Accuracy: 0.9100\n",
      "Loss: 0.015781400725245476\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.8900\n",
      "Loss: 0.013003232888877392\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  34,  29, 121]))\n",
      "Accuracy: 0.8800\n",
      "Loss: 0.011081460863351822\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.008848470635712147\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([266,  41, 469,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.008351737633347511\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.8900\n",
      "Loss: 0.005855238065123558\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9200\n",
      "Loss: 0.004545015282928944\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.004604425746947527\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  15,  25,  35,  29, 120]))\n",
      "Accuracy: 0.9300\n",
      "Loss: 0.003072644816711545\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9000\n",
      "Loss: 0.0023283257614821196\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9100\n",
      "Loss: 0.001764403423294425\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([265,  41, 470,  16,  24,  35,  29, 120]))\n",
      "Accuracy: 0.9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(40)):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(r8.data)\n",
    "    # We might want to use the \"weight\" parameter for the loss with unbalanced dataset\n",
    "    # since with a low learning rate the model just assigns every doc to class \"earn\"\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    loss = F.cross_entropy(out[r8.data.train_mask], r8.data.y[r8.data.train_mask])\n",
    "    print('Loss:', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(np.unique(out[r8.data.train_mask].max(dim=1)[1].detach().cpu().numpy(), return_counts=True))\n",
    "    #print(np.unique(r8.data.y[r8.data.train_mask].detach().cpu().numpy(), return_counts=True))\n",
    "    eval(model, r8.data, r8.data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 8,
   "id": "bf770635",
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy: 0.2894\n"
     ]
    }
   ],
   "source": [
    "eval(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
=======
      "Accuracy: 0.9000\n"
>>>>>>> main
     ]
    }
   ],
   "source": [
    "eval(model, r8.data, r8.data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "71d5323e",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
