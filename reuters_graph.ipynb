{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters Graph version\n",
    "\n",
    "### Sources\n",
    "- [Philipp tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html)\n",
    "- [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "- [nltk docs](https://www.nltk.org/book/ch02.html)\n",
    "\n",
    "### TODO\n",
    "- make edge_list a set and then a list again\n",
    "- edges: tf.idf and word-word PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floor_vl/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/floor_vl/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from datasets.graph_utils import PMI, tf_idf_mtx\n",
    "\n",
    "def prepare_reuters(r8=False):\n",
    "    \"\"\"Filters out all documents which have more or less than 1 class. Then filters out all classes which have no remaining documents.\n",
    "\n",
    "    Args:\n",
    "        r8 (bool, optional): R8 is constructed by taking only the top 10 (original) classes. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        train_docs (list): List of training documents.\n",
    "        test_docs (list): List of test documents.\n",
    "    \"\"\"    \n",
    "    # Filter out docs which don't have exactly 1 class\n",
    "    data = defaultdict(lambda: {'train': [], 'test': []})\n",
    "    for doc in reuters.fileids():\n",
    "        # print(\"reuter field=\", doc)\n",
    "        if len(reuters.categories(doc)) == 1:\n",
    "            if doc.startswith('training'):\n",
    "                data[reuters.categories(doc)[0]]['train'].append(doc)\n",
    "            elif doc.startswith('test'):\n",
    "                data[reuters.categories(doc)[0]]['test'].append(doc)\n",
    "            else:\n",
    "                print(doc)\n",
    "\n",
    "    # Filter out classes which have no remaining docs\n",
    "    for cls in reuters.categories():\n",
    "        if len(data[cls]['train']) < 1 or len(data[cls]['test']) < 1:\n",
    "            data.pop(cls, None)\n",
    "\n",
    "    if r8:\n",
    "        # Choose top 10 classes and then select the ones which still remain after filtering\n",
    "        popular = sorted(reuters.categories(), key=lambda cls: len(reuters.fileids(cls)), reverse=True)[:10]\n",
    "        data = dict([(cls, splits) for (cls, splits) in data.items() if cls in popular])\n",
    "\n",
    "    # Create splits\n",
    "    train_docs = [doc for cls, splits in data.items() for doc in splits['train']]\n",
    "    test_docs = [doc for cls, splits in data.items() for doc in splits['test']]\n",
    "\n",
    "    return train_docs, test_docs, list(data.keys())\n",
    "\n",
    "class Reuters:\n",
    "    def __init__(self, r8, device, val_size=0.1):\n",
    "        self.device = device\n",
    "        print('Prepare Reuters dataset')\n",
    "        train_docs, test_docs, classes = prepare_reuters(r8)\n",
    "        corpus = [[word.lower() for word in reuters.words(doc)] for doc in train_docs + test_docs]\n",
    "        \n",
    "        print('Compute tf.idf')\n",
    "        tf_idf, words = tf_idf_mtx(corpus)\n",
    "        \n",
    "        print('Compute PMI scores')\n",
    "        pmi = PMI(corpus)\n",
    "        \n",
    "        # Index to node name mapping\n",
    "        self.iton = list(train_docs + test_docs + words)\n",
    "        # Node name to index mapping\n",
    "        self.ntoi = {self.iton[i]: i for i in range(len(self.iton))}\n",
    "        \n",
    "        # Edge index and values for dataset\n",
    "        print('Generate edges')\n",
    "        edge_index, edge_attr = self.generate_edges(len(train_docs + test_docs), tf_idf, pmi)\n",
    "        \n",
    "        # Index to label mapping\n",
    "        self.itol = classes\n",
    "        # Label in index mapping\n",
    "        self.loti = {self.itol[i]: i for i in range(len(self.itol))}\n",
    "        # Labels to node mapping, where word nodes get the label of -1\n",
    "        ntol = [self.loti[reuters.categories(node)[0]] if reuters.categories(node) else -1 for node in self.iton]\n",
    "        ntol = torch.tensor(ntol, device=device)\n",
    "        \n",
    "        # Generate masks/splits\n",
    "        print('Generate masks')\n",
    "        train_mask, val_mask, test_mask = self.generate_masks(len(train_docs), len(test_docs), val_size)\n",
    "        \n",
    "        # Feature matrix is Identity (according to TextGCN)\n",
    "        print('Generate feature matrix')\n",
    "        node_feats = torch.eye(len(self.iton), device=self.device)\n",
    "        print('Features mtx is {} GBs in size'.format(node_feats.nelement() * node_feats.element_size() * 1e-9))\n",
    "        \n",
    "        # Create pytorch geometric format data\n",
    "        self.data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_attr, y=ntol)\n",
    "        self.data.train_mask = train_mask\n",
    "        self.data.val_mask = val_mask\n",
    "        self.data.test_mask = test_mask\n",
    "        \n",
    "    def generate_edges(self, num_docs, tf_idf, pmi):\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        # Document-word edges\n",
    "        for d_ind, doc in enumerate(tf_idf):\n",
    "            word_inds = doc.indices\n",
    "            for w_ind in word_inds:\n",
    "                edge_index.append([d_ind, num_docs + w_ind])\n",
    "                edge_index.append([num_docs + w_ind, d_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "                edge_attr.append(tf_idf[d_ind, w_ind])\n",
    "        \n",
    "        # Word-word edges\n",
    "        for (word_i, word_j), score in pmi.items():\n",
    "            w_i_ind = self.ntoi[word_i]\n",
    "            w_j_ind = self.ntoi[word_j]\n",
    "            edge_index.append([w_i_ind, w_j_ind])\n",
    "            edge_index.append([w_j_ind, w_i_ind])\n",
    "            edge_attr.append(score)\n",
    "            edge_attr.append(score)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr)\n",
    "        return edge_index, edge_attr\n",
    "    \n",
    "    def generate_masks(self, train_num, test_num, val_size):\n",
    "        # Mask all non-training docs\n",
    "        train_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        train_mask[:train_num] = 1\n",
    "        \n",
    "        # Randomly select val docs from train-docs and mask accordingly\n",
    "        val_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        val_mask_inds = torch.randperm(train_num)[:int(val_size * train_num)]\n",
    "        val_mask[val_mask_inds] = 1\n",
    "        train_mask[val_mask_inds] = 0\n",
    "        \n",
    "        # Mask all non-test docs\n",
    "        test_mask = torch.zeros(len(self.iton), device=self.device)\n",
    "        test_mask[train_num:test_num] = 1\n",
    "        test_mask = train_mask.bool()\n",
    "\n",
    "        return train_mask.bool(), val_mask.bool(), test_mask.bool()\n",
    "        \n",
    "        \n",
    "class R52(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=False, device=device, val_size=val_size)\n",
    "        \n",
    "        \n",
    "class R8(Reuters):\n",
    "    def __init__(self, device, val_size=0.1):\n",
    "        super().__init__(r8=True, device=device, val_size=val_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare Reuters dataset\n",
      "Compute tf.idf\n",
      "Compute PMI scores\n",
      "Generate edges\n",
      "Generate masks\n",
      "Generate feature matrix\n",
      "Features mtx is 4.347028624 GBs in size\n"
     ]
    }
   ],
   "source": [
    "r8 = R8(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[4547910], edge_index=[2, 4547910], test_mask=[32966], train_mask=[32966], val_mask=[32966], x=[32966, 32966], y=[32966])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r8.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r8.data.val_mask * r8.data.train_mask * r8.data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.utils.is_undirected(r8.data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cora_dataset = torch_geometric.datasets.Planetoid(root='/tmp/cora', name=\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(len(r8.iton), 8)\n",
    "        self.conv2 = GCNConv(8, 8)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x.double(), data.edge_index, data.edge_attr\n",
    "        print(x.type())\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        print(x.type())\n",
    "        x = F.relu(x)\n",
    "        print(x.type())\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        print(x.type())\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        print(x.type())\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def eval(model, data, mask):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = int(pred[mask].eq(data.y[mask]).sum().item())\n",
    "    acc = correct / int(mask.sum())\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[4547910], edge_index=[2, 4547910], test_mask=[32966], train_mask=[32966], val_mask=[32966], x=[32966, 32966], y=[32966])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "data = r8.data\n",
    "\n",
    "r8.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, data, data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d351e6c963b4dcdaa36ad526198c416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "Accuracy: 0.2964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    eval(model, data, data.val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2894\n"
     ]
    }
   ],
   "source": [
    "eval(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
